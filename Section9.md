# 41. Vector Reduction using global memory only (baseline)

## 1. 벡터 리덕션(Vector Reduction)의 정의

* **개념:** 대규모 벡터의 모든 요소를 하나의 값(합계 등)으로 줄여나가는 병렬 알고리즘입니다.
* **특징:** 두 벡터를 더하는 '벡터 덧셈'과 달리, **단일 벡터** 내에서 연산이 이루어지며 단계마다 데이터의 양이 절반씩 줄어듭니다.
* **용도:** 행렬 곱셈, 통계 계산 등 다양한 GPU 연산의 핵심 요소입니다.

---

## 2. 핵심 알고리즘: 트리 기반 접근 (Tree-based Approach)

이 방식은 데이터를 마치 거꾸로 된 나무 모양처럼 단계적으로 합쳐 나갑니다.

* **보폭(Stride)의 원리:**
* **Step 1:** 보폭이 1입니다. $input[0]$과 $input[1]$을 더합니다.
* **Step 2:** 보폭이 2입니다. $input[0]$과 $input[2]$를 더합니다.
* **Step 3:** 보폭이 4입니다. $input[0]$과 $input[4]$를 더합니다.


* 이처럼 보폭은 매 단계마다 **2배**씩 증가하고, 실제 연산에 참여하는 스레드 수는 **절반**씩 감소합니다.

---

## 3. 구현상의 주요 문제와 해결책

### ① 유휴 스레드(Idle Threads) 문제

연산 단계가 진행될수록 필요한 스레드 수는 줄어듭니다. 모든 스레드를 활성화하면 불필요한 연산이 발생하고 메모리 오류가 날 수 있습니다.

* **해결책 (필터링):** `if (tid % (2 * stride) == 0)` 조건을 사용해 현재 단계에서 연산이 필요한 짝수 번째 스레드만 골라냅니다.

### ② 메모리 경계 확인 (Memory Violation)

스레드가 `index + stride` 위치의 데이터를 참조할 때, 벡터의 크기()를 벗어나면 에러가 발생합니다.

* **해결책:** `if (index + stride < n)` 조건을 추가하여 안전한 범위 내에서만 메모리에 접근하도록 합니다.

### ③ 블록 간 동기화 문제 (Global Synchronization)

CUDA는 블록(Block) 간의 실시간 동기화를 지원하지 않습니다.

* **해결책 (커널 분리):**
1. **Kernel 1:** 각 블록이 담당 영역의 **부분 합(Partial Sums)**을 구합니다.
2. **연속 배치:** 각 블록의 결과값을 벡터의 앞부분(`input[blockIdx.x]`)에 모읍니다.
3. **Kernel 2:** 모인 부분 합들을 입력으로 하여 최종 합계가 나올 때까지 다시 리덕션을 실행합니다.



---

## 4. 최종 코드 논리 구조

```cpp
__global__ void reduce_inplace(int *input, int n) {
    unsigned int tid = threadIdx.x;
    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;

    // 1. 단계별 리덕션 (for 루프)
    for (unsigned int stride = 1; stride < blockDim.x; stride *= 2) {
        // 짝수 보폭 스레드만 연산 수행
        if (i + stride < n && tid % (2 * stride) == 0) {
            input[i] += input[i + stride];
        }
        // 다음 단계로 넘어가기 전, 블록 내 모든 스레드 동기화
        __syncthreads();
    }

    // 2. 각 블록의 최종 부분 합을 벡터의 앞부분으로 수집
    if (tid == 0) {
        input[blockIdx.x] = input[i];
    }
}

```

---

## 5. 요약 및 시사점

* **동기화:** `__syncthreads()`는 이전 단계의 연산 결과가 메모리에 완전히 기록된 후 다음 단계로 넘어가게 해주는 필수 장치입니다.
* **효율성:** 위 코드는 전역 메모리(Global Memory)만 사용하여 구현이 단순하지만, 성능 최적화(공유 메모리 사용, 분기 예측 개선 등)의 여지가 많이 남아 있습니다.

# 42. Understanding the code and the profiling of the vector reduction

## 1. CPU 검증 함수 작성 (`CPU_reduce`)

GPU 커널의 정확성을 확인하기 위해 동일한 작업을 수행하는 CPU용 함수를 작성합니다.

* **입력/출력:** 여러 요소를 가진 벡터(float*)를 입력받아 하나의 합계값(float)을 반환합니다.
* **역할:** GPU 결과값과 비교하여 알고리즘의 **정확성(Correctness)**을 검증하는 기준점(Golden Reference)이 됩니다.

## 2. 메인 함수(`main`) 및 CUDA 초기화 (7단계 원칙)

CUDA 애플리케이션의 표준 절차에 따라 메모리를 할당하고 데이터를 준비합니다.

1. **메모리 할당:** `malloc` 또는 `new`(호스트용)와 `cudaMalloc`(디바이스용)을 사용하여  (1024x1024) 크기의 공간을 할당합니다.
2. **데이터 초기화:** 호스트(CPU)에서 입력 벡터에 초기값을 할당합니다.
3. **데이터 복사:** `cudaMemcpy`를 사용하여 데이터를 호스트에서 디바이스(GPU)로 전송합니다. (`cudaMemcpyHostToDevice`)

## 3. GPU 커널 구성 및 다단계 실행

벡터 리덕션은 한 번의 커널 호출로 끝나지 않고, 데이터가 충분히 줄어들 때까지 여러 번 호출해야 합니다.

* **블록 및 그리드 설정:** 블록 크기는 256개 스레드로 고정하고, 그리드 크기는 $N/\text{BlockSize}$로 계산합니다.
* **다단계 리덕션 과정:**
1. **1차 실행:** 100만 개 요소를 처리하여 4,096개의 부분 합(Partial Sums)을 생성.
2. **2차 실행:** 4,096개의 부분 합을 입력으로 받아 16개의 부분 합을 생성.
3. **3차 실행:** 16개의 요소를 처리하여 최종 1개의 합계를 생성 (단일 블록 사용).


* **동기화:** 각 커널 실행 후 `cudaDeviceSynchronize()`를 호출하여 GPU 작업이 완료될 때까지 CPU가 기다리도록 합니다.

## 4. 결과 확인 및 자원 해제

* **결과 복사:** 디바이스 메모리의 첫 번째 요소(최종 합계)를 호스트로 다시 복사합니다. (`cudaMemcpyDeviceToHost`)
* **오차 확인:** CPU 결과와 GPU 결과가 소수점 아래에서 미세하게 다를 수 있는데, 이는 **부동 소수점 연산 정밀도(Precision)** 차이 때문입니다. (더 높은 정밀도가 필요하면 `double` 타입 사용 권장)
* **메모리 해제:** `cudaFree`와 `delete`를 사용하여 할당된 메모리를 정리합니다.

## 5. Nsight Compute를 이용한 프로파일링 분석

강의 후반부에서는 성능 분석 도구인 **Nsight Compute**를 사용하여 커널을 분석합니다.

* **처리 시간:** 첫 번째 커널(4,096개 블록)이 가장 오래 걸리며, 단계가 진행될수록 데이터가 줄어들어 실행 시간이 단축됩니다.
* **하드웨어 활용도:** * **L1/L2 캐시 히트율:** 벡터 덧셈(Vector Addition)과 달리 리덕션은 동일 요소를 재사용하므로 캐시 히트율이 높게 나타납니다 (L1 약 91%, L2 약 80%).
* **리소스 점유:** 첫 번째 커널은 많은 블록을 SM(Streaming Multiprocessor)에 할당하여 실행되지만, 마지막 커널은 1개의 블록만 사용하므로 GPU 자원의 극히 일부만 활용하게 됩니다.
