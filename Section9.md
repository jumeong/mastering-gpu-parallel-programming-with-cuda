# 41. Vector Reduction using global memory only (baseline)

## 1. 벡터 리덕션(Vector Reduction)의 정의

* **개념:** 대규모 벡터의 모든 요소를 하나의 값(합계 등)으로 줄여나가는 병렬 알고리즘입니다.
* **특징:** 두 벡터를 더하는 '벡터 덧셈'과 달리, **단일 벡터** 내에서 연산이 이루어지며 단계마다 데이터의 양이 절반씩 줄어듭니다.
* **용도:** 행렬 곱셈, 통계 계산 등 다양한 GPU 연산의 핵심 요소입니다.

---

## 2. 핵심 알고리즘: 트리 기반 접근 (Tree-based Approach)

이 방식은 데이터를 마치 거꾸로 된 나무 모양처럼 단계적으로 합쳐 나갑니다.

* **보폭(Stride)의 원리:**
* **Step 1:** 보폭이 1입니다. $input[0]$과 $input[1]$을 더합니다.
* **Step 2:** 보폭이 2입니다. $input[0]$과 $input[2]$를 더합니다.
* **Step 3:** 보폭이 4입니다. $input[0]$과 $input[4]$를 더합니다.


* 이처럼 보폭은 매 단계마다 **2배**씩 증가하고, 실제 연산에 참여하는 스레드 수는 **절반**씩 감소합니다.

---

## 3. 구현상의 주요 문제와 해결책

### ① 유휴 스레드(Idle Threads) 문제

연산 단계가 진행될수록 필요한 스레드 수는 줄어듭니다. 모든 스레드를 활성화하면 불필요한 연산이 발생하고 메모리 오류가 날 수 있습니다.

* **해결책 (필터링):** `if (tid % (2 * stride) == 0)` 조건을 사용해 현재 단계에서 연산이 필요한 짝수 번째 스레드만 골라냅니다.

### ② 메모리 경계 확인 (Memory Violation)

스레드가 `index + stride` 위치의 데이터를 참조할 때, 벡터의 크기()를 벗어나면 에러가 발생합니다.

* **해결책:** `if (index + stride < n)` 조건을 추가하여 안전한 범위 내에서만 메모리에 접근하도록 합니다.

### ③ 블록 간 동기화 문제 (Global Synchronization)

CUDA는 블록(Block) 간의 실시간 동기화를 지원하지 않습니다.

* **해결책 (커널 분리):**
1. **Kernel 1:** 각 블록이 담당 영역의 **부분 합(Partial Sums)**을 구합니다.
2. **연속 배치:** 각 블록의 결과값을 벡터의 앞부분(`input[blockIdx.x]`)에 모읍니다.
3. **Kernel 2:** 모인 부분 합들을 입력으로 하여 최종 합계가 나올 때까지 다시 리덕션을 실행합니다.



---

## 4. 최종 코드 논리 구조

```cpp
__global__ void reduce_inplace(int *input, int n) {
    unsigned int tid = threadIdx.x;
    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;

    // 1. 단계별 리덕션 (for 루프)
    for (unsigned int stride = 1; stride < blockDim.x; stride *= 2) {
        // 짝수 보폭 스레드만 연산 수행
        if (i + stride < n && tid % (2 * stride) == 0) {
            input[i] += input[i + stride];
        }
        // 다음 단계로 넘어가기 전, 블록 내 모든 스레드 동기화
        __syncthreads();
    }

    // 2. 각 블록의 최종 부분 합을 벡터의 앞부분으로 수집
    if (tid == 0) {
        input[blockIdx.x] = input[i];
    }
}

```

---

## 5. 요약 및 시사점

* **동기화:** `__syncthreads()`는 이전 단계의 연산 결과가 메모리에 완전히 기록된 후 다음 단계로 넘어가게 해주는 필수 장치입니다.
* **효율성:** 위 코드는 전역 메모리(Global Memory)만 사용하여 구현이 단순하지만, 성능 최적화(공유 메모리 사용, 분기 예측 개선 등)의 여지가 많이 남아 있습니다.

# 42. Understanding the code and the profiling of the vector reduction

## 1. CPU 검증 함수 작성 (`CPU_reduce`)

GPU 커널의 정확성을 확인하기 위해 동일한 작업을 수행하는 CPU용 함수를 작성합니다.

* **입력/출력:** 여러 요소를 가진 벡터(float*)를 입력받아 하나의 합계값(float)을 반환합니다.
* **역할:** GPU 결과값과 비교하여 알고리즘의 **정확성(Correctness)**을 검증하는 기준점(Golden Reference)이 됩니다.

## 2. 메인 함수(`main`) 및 CUDA 초기화 (7단계 원칙)

CUDA 애플리케이션의 표준 절차에 따라 메모리를 할당하고 데이터를 준비합니다.

1. **메모리 할당:** `malloc` 또는 `new`(호스트용)와 `cudaMalloc`(디바이스용)을 사용하여  (1024x1024) 크기의 공간을 할당합니다.
2. **데이터 초기화:** 호스트(CPU)에서 입력 벡터에 초기값을 할당합니다.
3. **데이터 복사:** `cudaMemcpy`를 사용하여 데이터를 호스트에서 디바이스(GPU)로 전송합니다. (`cudaMemcpyHostToDevice`)

## 3. GPU 커널 구성 및 다단계 실행

벡터 리덕션은 한 번의 커널 호출로 끝나지 않고, 데이터가 충분히 줄어들 때까지 여러 번 호출해야 합니다.

* **블록 및 그리드 설정:** 블록 크기는 256개 스레드로 고정하고, 그리드 크기는 $N/\text{BlockSize}$로 계산합니다.
* **다단계 리덕션 과정:**
1. **1차 실행:** 100만 개 요소를 처리하여 4,096개의 부분 합(Partial Sums)을 생성.
2. **2차 실행:** 4,096개의 부분 합을 입력으로 받아 16개의 부분 합을 생성.
3. **3차 실행:** 16개의 요소를 처리하여 최종 1개의 합계를 생성 (단일 블록 사용).


* **동기화:** 각 커널 실행 후 `cudaDeviceSynchronize()`를 호출하여 GPU 작업이 완료될 때까지 CPU가 기다리도록 합니다.

## 4. 결과 확인 및 자원 해제

* **결과 복사:** 디바이스 메모리의 첫 번째 요소(최종 합계)를 호스트로 다시 복사합니다. (`cudaMemcpyDeviceToHost`)
* **오차 확인:** CPU 결과와 GPU 결과가 소수점 아래에서 미세하게 다를 수 있는데, 이는 **부동 소수점 연산 정밀도(Precision)** 차이 때문입니다. (더 높은 정밀도가 필요하면 `double` 타입 사용 권장)
* **메모리 해제:** `cudaFree`와 `delete`를 사용하여 할당된 메모리를 정리합니다.

## 5. Nsight Compute를 이용한 프로파일링 분석

강의 후반부에서는 성능 분석 도구인 **Nsight Compute**를 사용하여 커널을 분석합니다.

* **처리 시간:** 첫 번째 커널(4,096개 블록)이 가장 오래 걸리며, 단계가 진행될수록 데이터가 줄어들어 실행 시간이 단축됩니다.
* **하드웨어 활용도:** * **L1/L2 캐시 히트율:** 벡터 덧셈(Vector Addition)과 달리 리덕션은 동일 요소를 재사용하므로 캐시 히트율이 높게 나타납니다 (L1 약 91%, L2 약 80%).
* **리소스 점유:** 첫 번째 커널은 많은 블록을 SM(Streaming Multiprocessor)에 할당하여 실행되지만, 마지막 커널은 1개의 블록만 사용하므로 GPU 자원의 극히 일부만 활용하게 됩니다.

# 43. Optimizing the vector reduction (removing the filter)
## 1. 제어 구조 개선: `while` 루프 도입

기존의 고정된 단계를 거치는 대신, 데이터 크기에 따라 동적으로 작동하는 `while` 루프를 도입했습니다.

* **조건:** 그리드 크기(Grid Size)가 1보다 큰 동안 커널을 반복 실행합니다.
* **흐름:** 1. 데이터가 100만 개일 때, 첫 번째 단계 후 그리드 크기는 4096이 됩니다.
2. 두 번째 단계 계산 결과 그리드 크기는 16이 됩니다.
3. 세 번째 단계에서 그리드 크기가 1이 되면 루프를 빠져나와 최종 합산을 수행합니다.
* **결과:** CPU 결과와 비교했을 때 동일한 결과가 나옴을 확인하여 로직의 정당성을 증명했습니다.

---

## 2. 핵심 최적화: 필터(조건문) 제거

가장 중요한 부분은 커널 내부의 **`if` 필터(짝수 번째 쓰레드만 동작하게 하는 조건문)를 제거**한 것입니다.

### 필터 제거가 가능한 이유

* **기존 방식:** `if (tid % (2 * stride) == 0)` 조건을 사용해 특정 쓰레드만 연산에 참여시켰습니다.
* **제거 후:** 모든 쓰레드가 연산을 수행하게 되어 일부 메모리 값이 의도치 않게 변하지만, **스트라이드(Stride)** 구조 덕분에 다음 단계에서 해당 값들을 참조하지 않습니다. 따라서 최종 결과값에는 영향을 주지 않습니다.

---

## 3. 성능 개선 분석 (NVIDIA Nsight Compute)

필터를 제거한 후 성능이 비약적으로 향상되었습니다.

* **실행 시간:** 약 **283㎲ → 176㎲ (약 40% 단축)**.
* **이유:** `%`(나머지 연산, Modulus)는 GPU에서 매우 **비싼(Expensive) 연산**이기 때문입니다.
* 덧셈/곱셈: 약 2~9 사이클 소요.
* 나머지/나눗셈: 약 **290 사이클** 소요.


* **어셈블리 분석:** * 필터가 있을 때: 조건문 처리를 위해 약 50~60개의 어셈블리 명령어가 생성됨.
* 필터가 없을 때: 단 6개의 명령어로 줄어듦.



---

## 4. 디버깅을 통한 시각적 증명

Visual Studio CUDA 디버거를 통해 메모리 변화를 직접 확인했습니다.

* **필터가 있을 때:** 짝수 번째 요소만 값이 변하고 홀수 번째는 그대로 유지됨.
* **필터가 없을 때:** 모든 요소의 값이 변함 (모든 쓰레드가 연산을 수행했음을 증명).
* **결론:** 모든 요소가 변하더라도 최종 합산 로직(스트라이드)에는 문제가 없으며, 비싼 연산을 제거함으로써 성능만 크게 이득을 보게 됨.

---

### 요약 테이블

| 항목 | 최적화 전 (Baseline) | 최적화 후 |
| --- | --- | --- |
| **조건문 (`if`)** | 있음 (나머지 연산 포함) | **없음 (제거됨)** |
| **실행 시간** | 283 ㎲ | **176 ㎲ (40% 향상)** |
| **주요 병목** | `%` 연산의 높은 사이클 소요 | 메모리 로드(Load) 단계 |
| **데이터 정확도** | 정확함 | **정확함 (영향 없음)** |


# 44. The Race Condition and the debugging option

## 1. 배경: 왜 결과값이 다르게 나오는가?

한 학생이 강의와 동일한 코드를 실행했음에도 다른 결과가 나온다는 의문을 제기했습니다. 조사 결과, 원인은 **`-G` (디버깅 정보 생성) 옵션**의 유무에 있었습니다.

* **Linux:** 명령줄에서 직접 옵션을 추가/제거하기 쉬움.
* **Windows (Visual Studio):** 디폴트 설정이 숨겨져 있어 주의가 필요함.
* `Project Properties` -> `CUDA C/C++` -> `Device` -> `Generate GPU Debug Information` 설정이 이에 해당합니다.



---

## 2. 핵심 개념: 레이스 컨디션 (Race Condition)

벡터 리덕션에서 필터(`if` 조건문)를 제거했을 때 발생하는 문제입니다.

* **현상:** 여러 쓰레드가 동시에 동일한 메모리 위치를 읽고 쓰려고 할 때, **실행 순서에 따라 결과가 달라지는 현상**입니다.
* **예시:**
1. 쓰레드 1이 `A[1]`의 값을 읽어 `A[0]`에 저장하려고 함.
2. 동시에 쓰레드 2가 `A[1]`의 값을 수정하려고 함.
3. 쓰레드 2가 먼저 수정해버리면, 쓰레드 1은 업데이트된(잘못된) 값을 읽어 `A[0]`에 반영하게 됩니다.



---

## 3. `-G` 옵션의 비밀: 최적화 비활성화

스크립트에서 가장 강조하는 부분입니다. CUDA 컴파일러 드라이버(nvcc) 매뉴얼에 따르면:

> **`-G` 옵션을 사용하면 디바이스 코드의 모든 최적화(Optimizations)가 해제됩니다.**

* **최적화 활성 시 (기본값):** 컴파일러가 병렬성을 높이기 위해 명령어 순서를 재배치(Reordering)하며, 이 과정에서 레이스 컨디션이 발생해 **틀린 결과**가 나올 수 있습니다.
* **최적화 비활성 시 (`-G` 추가):** 명령어가 순차적으로 실행될 가능성이 높아져 레이스 컨디션이 억제되고, 운 좋게 **정확한 결과**가 나올 수 있습니다.

---

## 4. 실습 결과 비교

강사는 Windows와 Linux 환경 모두에서 이를 증명했습니다.

| 환경 | 옵션 미적용 (Optimized) | `-G` 옵션 적용 (Debug mode) |
| --- | --- | --- |
| **결과값** | CPU 결과와 크게 다름 (오차 발생) | CPU 결과와 거의 동일함 |
| **이유** | 최적화로 인한 레이스 컨디션 발생 | 최적화 해제로 레이스 컨디션 억제 |
| **성능** | 빠름 | 느림 (디버깅 정보 포함 및 최적화 부재) |

---

## 5. 결론 및 요약

* **주의사항:** 디버깅 모드에서 결과가 맞다고 해서 코드가 완벽한 것은 아닙니다. 이는 최적화가 꺼져서 우연히 맞은 것일 뿐, 실제 배포용(Release) 빌드에서는 레이스 컨디션으로 인해 문제가 발생할 수 있습니다.
* **학습 포인트:** 하드웨어 아키텍처에 맞는 `Compute Capability`(예: RTX 3060은 80)를 설정하는 것도 중요합니다.
