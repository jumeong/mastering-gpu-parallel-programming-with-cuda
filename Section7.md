# 38. The shared memory

## 1. GPU 메모리 계층 구조 요약

GPU의 메모리는 접근 범위와 속도에 따라 여러 단계로 나뉩니다.

* **Global Memory (전역 메모리):** 모든 스레드와 CPU가 접근 가능. 용량이 크지만 지연 시간(Latency)이 가장 김 (**300+ cycles**).
* **L2 Cache:** 모든 SM(Streaming Multiprocessor)이 공유. Global Memory보다 빠름 (**~200 cycles**).
* **L1 Cache & Shared Memory:** SM 내부에 위치. 매우 빠름 (**L1: ~33 cycles, Shared: ~25 cycles**).
* **Registers (레지스터):** 각 스레드별 개별 공간. 가장 빠름.

---

## 2. Shared Memory의 특징 (Software Cache)

Shared Memory는 L1 캐시와 물리적으로 같은 위치에 있지만, 동작 방식이 다릅니다.

* **가시성:** 같은 블록(Block) 내의 스레드들끼리 데이터를 공유하고 통신할 수 있음.
* **프로그래밍 가능:** 개발자가 직접 변수를 할당(`__shared__`)하고 제어할 수 있는 "Software Cache"임. (L1은 하드웨어가 자동으로 관리)
* **용도:** 데이터 재사용(Data Reuse)을 통해 Global Memory 트래픽을 줄이고 성능 병목 현상을 해결함.
* **속도:** L1(33 cycles)보다 Shared Memory(25 cycles)가 더 빠른데, 이는 L1 캐시가 수행하는 태그 검색(Tag Search) 등의 하드웨어 로직이 Shared Memory에는 필요 없기 때문임.

### [물리적 통합 구조: Ampere(A100) 아키텍처 예시]

최신 아키텍처에서는 L1과 Shared Memory가 하나의 물리적 단위(예: 192KB)로 통합되어 있습니다.

* **가변 설정:** 컴파일러나 설정을 통해 L1과 Shared Memory의 비율을 조정할 수 있음. (예: Shared 100KB 설정 시 L1은 92KB 사용)

---

## 3. Shared Memory의 하드웨어 구조: Banks (뱅크)

Shared Memory는 효율적인 병렬 접근을 위해 **Bank**라는 단위로 나뉩니다.

* **Cache Line (Row):** 한 행은 128 Bytes.
* **Bank (Column):** 한 행은 **32개의 뱅크**로 나뉨. (128B / 32 = **4 Bytes당 1개 뱅크**)
* **접근 규칙:** 사이클당 각 뱅크에서 4바이트씩 접근 가능. 32개 뱅크가 서로 다르면 한 사이클에 128바이트를 동시에 읽을 수 있음.

---

## 4. Bank Conflict (뱅크 충돌)

성능 저하의 주요 원인인 뱅크 충돌을 이해하는 것이 중요합니다.

* **정의:** 여러 스레드가 **동일한 사이클에 동일한 뱅크(열) 내의 서로 다른 데이터**에 접근하려고 할 때 발생.
* **결과:** 충돌이 발생하면 접근이 직렬화(Serialized)되어 완료될 때까지 더 많은 사이클이 소모됨.
* **Broadcasting (브로드캐스팅):** 여러 스레드가 동일한 뱅크의 **"정확히 같은 데이터"**를 읽을 때는 충돌 없이 한 번에 처리됨.

### [충돌 예시 및 계산]

1. **연속 접근 (No Conflict):** Thread 0 -> Bank 0, Thread 1 -> Bank 1... (32개 뱅크 모두 다름) → **1 Cycle**
2. **Stride 접근 (Conflict 발생):** 16-byte stride로 접근 시, 여러 스레드가 동일한 열(Bank)에 배치됨.
* 한 열에 4개의 스레드가 몰린다면: **4 Cycles** 소요.


3. **Double Precision (8-byte):** 각 스레드가 8바이트를 읽으므로 기본적으로 2개의 뱅크를 점유함. 따라서 최소 2 사이클이 필요할 수 있음.

---

## 5. 요약 및 결론

* **Shared Memory**는 데이터 재사용이 빈번한 알고리즘(예: Vector Reduction)에서 성능을 최적화하는 핵심 도구입니다.
* **Bank Conflict**를 피하기 위해 스레드가 뱅크를 골고루 점유하도록 데이터 접근 패턴(Stride)을 설계해야 합니다.
* 성능 분석 시 **Nsight Compute**와 같은 툴을 활용하여 실제 뱅크 충돌 메트릭을 확인하는 것이 좋습니다.

# 39. Warp Divergence
## 1. 기본 개념 복습

* **SIMT 모델:** GPU는 '단일 명령 집합 다중 스레드(Single Instruction, Multiple Threads)' 방식으로 동작합니다. 즉, 하나의 **Warp(32개 스레드)**는 한 번에 동일한 명령어를 동시에 실행하는 것이 이상적입니다.
* **Warp Scheduler:** 스트리밍 멀티프로세서(SM) 내에서 워프들을 관리하고 하드웨어 자원에 할당하는 역할을 합니다.
* **Ideal Scenario (이상적 상황):** 워프 내 모든 32개 스레드가 동일한 명령어를 동시에 실행할 때 최고의 성능이 나옵니다.

---

## 2. Warp Divergence (워프 분기)란?

커널 코드 내에 `if-else`와 같은 조건문이 있을 때, 워프 내 스레드들이 서로 다른 실행 경로(Path)를 선택하게 되는 현상을 말합니다.

* **발생 원인:** 스레드 ID 등에 따라 조건문의 결과가 달라져, 일부 스레드는 `if`절을, 일부는 `else`절을 실행해야 할 때 발생합니다.
* **Serialization (직렬화):** 하드웨어는 서로 다른 두 경로를 동시에 실행할 수 없습니다. 따라서 `if` 경로를 먼저 실행하고(이때 `else` 스레드는 대기), 그 다음 `else` 경로를 실행(이때 `if` 스레드는 대기)하는 방식으로 **직렬화**됩니다.

---

## 3. 분기 수준별 시나리오

| 구분 | 특징 | 성능 영향 |
| --- | --- | --- |
| **No Divergence** | 모든 스레드가 같은 경로 실행 | 최상의 성능 (100% 효율) |
| **Moderate (보통)** | 절반은 A 경로, 절반은 B 경로 실행 | 실행 시간이 약 2배로 증가 |
| **Severe (심각)** | 스레드별로 미세하게 경로가 나뉨 (예: 4개 단위) | 경로의 개수만큼 실행 단계가 늘어나 성능 급락 |

> **중요 포인트:** 벡터 경계 검사(`if (idx < N)`)처럼 워프 전체 중 단 한 개의 워프에서만 마지막 몇 개 스레드가 분기되는 경우는 전체 성능에 큰 지장을 주지 않습니다. 진짜 문제는 워프 **내부**에서 스레드들이 쪼개지는 경우입니다.

---

## 4. Warp Divergence가 성능에 미치는 영향

1. **Serialization (직렬화):** 병렬로 처리될 작업이 순차적으로 처리되어 전체 실행 시간이 늘어납니다.
2. **Resource Waste (자원 낭비):** 특정 경로가 실행되는 동안 다른 경로의 스레드들은 **Idle(유휴)** 상태가 되며, 이를 'Masking(마스킹)' 처리하여 자원을 낭비하게 됩니다.
3. **Throughput 감소:** 사이클당 활성 스레드 수가 줄어들어 GPU의 실질적인 연산 처리량이 떨어집니다.

---

## 5. 결론 및 향후 과제

* Warp Divergence는 GPU 최적화에서 가장 우선적으로 고려해야 할 요소 중 하나입니다.
* **Vector Addition**과 같은 단순한 앱에서는 영향이 적을 수 있지만, **Vector Reduction(벡터 리덕션)**과 같이 복잡한 연산에서는 분기를 최적화하는 것만으로도 엄청난 성능 향상을 얻을 수 있습니다.
